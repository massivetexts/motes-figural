{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figural: Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "from figural.scoring import autoset_device, FiguralImage, preprocess_imlist, image_loader, collage, get_avg_sims, similarity_to_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP doesn't work on M1 GPUs yet; check here for updates: https://github.com/openai/CLIP/issues/247\n"
     ]
    }
   ],
   "source": [
    "device = autoset_device()\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>blank_sim</th>\n",
       "      <th>booklet</th>\n",
       "      <th>activity</th>\n",
       "      <th>Act_no</th>\n",
       "      <th>id</th>\n",
       "      <th>avg_sim</th>\n",
       "      <th>elaboration_raw</th>\n",
       "      <th>elaboration</th>\n",
       "      <th>pdf_path</th>\n",
       "      <th>titlepage</th>\n",
       "      <th>F</th>\n",
       "      <th>O</th>\n",
       "      <th>T</th>\n",
       "      <th>E</th>\n",
       "      <th>R</th>\n",
       "      <th>C</th>\n",
       "      <th>Name</th>\n",
       "      <th>testset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2573</th>\n",
       "      <td>../data/outputs/test1/activity1/6dcd4-8159.jpg</td>\n",
       "      <td>0.898965</td>\n",
       "      <td>BOOKLETA</td>\n",
       "      <td>activity1</td>\n",
       "      <td>1</td>\n",
       "      <td>6dcd4-8159</td>\n",
       "      <td>0.879442</td>\n",
       "      <td>5693</td>\n",
       "      <td>0.028185</td>\n",
       "      <td>../data/ttct_figural/Unmatched/Booklets/FS10 T...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>black eye creature</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            img_path  blank_sim   booklet  \\\n",
       "2573  ../data/outputs/test1/activity1/6dcd4-8159.jpg   0.898965  BOOKLETA   \n",
       "\n",
       "       activity  Act_no          id   avg_sim  elaboration_raw  elaboration  \\\n",
       "2573  activity1       1  6dcd4-8159  0.879442             5693     0.028185   \n",
       "\n",
       "                                               pdf_path  titlepage   F   O  \\\n",
       "2573  ../data/ttct_figural/Unmatched/Booklets/FS10 T...          1 NaN NaN   \n",
       "\n",
       "       T   E   R    C                Name  testset  \n",
       "2573 NaN NaN NaN  NaN  black eye creature    False  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/figural_ground_truth.csv', index_col=0)\n",
    "# 10% for test set\n",
    "test_prop = 0.1\n",
    "data['testset'] = (np.random.random(size=len(data)) < test_prop)\n",
    "data.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    1155\n",
       "2.0    1077\n",
       "1.0    1015\n",
       "Name: R, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[data.R <= 2]\n",
    "data['R'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3247it [00:05, 571.63it/s]\n"
     ]
    }
   ],
   "source": [
    "imloader = image_loader(data.img_path, contrast_factor=4, crop_bottom=True)\n",
    "image_inputs = preprocess_imlist(imloader, preprocess, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "text = clip.tokenize(data.Name.astype(str)).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_inputs)\n",
    "    text_features = model.encode_text(text)\n",
    " # normalize tensors\n",
    "image_features /= image_features.norm(dim=1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a single classifier for all activites using one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================TEXT======================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Not Original       0.77      0.46      0.57       123\n",
      "    Original       0.75      0.92      0.83       220\n",
      "\n",
      "    accuracy                           0.76       343\n",
      "   macro avg       0.76      0.69      0.70       343\n",
      "weighted avg       0.76      0.76      0.74       343\n",
      "\n",
      "=====================================IMAGE======================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Not Original       0.77      0.63      0.69       123\n",
      "    Original       0.81      0.90      0.85       220\n",
      "\n",
      "    accuracy                           0.80       343\n",
      "   macro avg       0.79      0.76      0.77       343\n",
      "weighted avg       0.80      0.80      0.79       343\n",
      "\n",
      "===================================IMAGE+TXT====================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Not Original       0.81      0.54      0.65       123\n",
      "    Original       0.78      0.93      0.85       220\n",
      "\n",
      "    accuracy                           0.79       343\n",
      "   macro avg       0.80      0.74      0.75       343\n",
      "weighted avg       0.79      0.79      0.78       343\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using one-hot\n",
    "enc = OneHotEncoder().fit(data.activity.values.reshape(-1, 1))\n",
    "enc.get_feature_names_out()\n",
    "cat_one_hot = enc.transform(data.activity.values.reshape(-1, 1)).toarray()\n",
    "\n",
    "for condition in ['text', 'image', 'image+txt']:\n",
    "    print(condition.upper().center(80, '='))\n",
    "    if condition == 'text':\n",
    "        embeds = text_features\n",
    "    elif condition == 'image':\n",
    "        embeds = image_features\n",
    "    elif condition == 'image+txt':\n",
    "        embeds = np.hstack([image_features, text_features])\n",
    "    else:\n",
    "        raise Exception('bad condition')\n",
    "\n",
    "    embeds = np.hstack([cat_one_hot, embeds])\n",
    "\n",
    "    matches = (~data['O'].isna())\n",
    "    train_X = embeds[(matches & ~data.testset)]\n",
    "    train_y = data.loc[(matches & ~data.testset), 'O']\n",
    "    test_X = embeds[(matches & data.testset)]\n",
    "    test_y = data.loc[(matches & data.testset), 'O']\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators = 100, random_state=0)\n",
    "    clf.fit(train_X, train_y)\n",
    "    y_pred = clf.predict(test_X)\n",
    "    print(metrics.classification_report(test_y, y_pred, target_names=['Not Original', 'Original']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">importance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>catlabel</th>\n",
       "      <td>0.000374</td>\n",
       "      <td>0.004118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img</th>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.586238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>txt</th>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.409643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         importance          \n",
       "               mean       sum\n",
       "label                        \n",
       "catlabel   0.000374  0.004118\n",
       "img        0.001145  0.586238\n",
       "txt        0.000800  0.409643"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featnames = ['catlabel'] * enc.get_feature_names_out().shape[0] + ['img'] * 512 + ['txt'] * 512\n",
    "x = pd.DataFrame(zip(featnames, clf.feature_importances_), columns=['label', 'importance'])\n",
    "x.groupby('label').aggregate(['mean', 'sum'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a different classifier for each activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================Predicting variable: O=============================\n",
      "--------------------------------------BAGG--------------------------------------\n",
      "                                   IMAGE+TXT                                    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:58<00:00,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    OVERALL                                     \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.75      0.76       123\n",
      "         1.0       0.86      0.88      0.87       220\n",
      "\n",
      "    accuracy                           0.83       343\n",
      "   macro avg       0.82      0.81      0.82       343\n",
      "weighted avg       0.83      0.83      0.83       343\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for targetvar in list(\"O\", \"R\"):\n",
    "    print(f\"Predicting variable: {targetvar}\".center(80,'='))\n",
    "    for classifier in ['rf']: #'ada', 'xgboost']:\n",
    "        print(classifier.upper().center(80, '-'))\n",
    "\n",
    "        for condition in ['image+txt']: #['text', 'image', 'image+txt']:\n",
    "            ytrue_collector = []\n",
    "            ypred_collector = []\n",
    "            label_collector = []\n",
    "\n",
    "            print(condition.upper().center(80, ' '))\n",
    "            if condition == 'text':\n",
    "                embeds = text_features\n",
    "            elif condition == 'image':\n",
    "                embeds = image_features\n",
    "            elif condition == 'image+txt':\n",
    "                embeds = np.hstack([image_features, text_features])\n",
    "            else:\n",
    "                raise Exception('bad condition')\n",
    "\n",
    "            for activity in tqdm(data.activity.unique()):\n",
    "                matches = (data.activity == activity) & (~pd.to_numeric(data[targetvar], errors='coerce').isna())\n",
    "                train_X = embeds[(matches & ~data.testset)]\n",
    "                train_y = data.loc[(matches & ~data.testset), targetvar] #.astype(bool)\n",
    "                test_X = embeds[(matches & data.testset)]\n",
    "                test_y = data.loc[(matches & data.testset), targetvar] #.astype(bool)\n",
    "\n",
    "                if classifier == 'rf':\n",
    "                    clf = RandomForestClassifier(n_estimators = 300)\n",
    "                elif classifier == 'xgboost':\n",
    "                    clf = XGBClassifier(n_estimators=300, learning_rate=0.2, objective='binary:logistic')\n",
    "                elif classifier == 'ada':\n",
    "                    clf = AdaBoostClassifier(n_estimators=100)\n",
    "                elif classifier == 'bagg':\n",
    "                    clf = BaggingClassifier(n_estimators=100)\n",
    "                clf.fit(train_X, train_y)\n",
    "                y_pred = clf.predict(test_X)\n",
    "\n",
    "                ypred_collector.append(y_pred)\n",
    "                ytrue_collector.append(test_y)\n",
    "                label_collector.append(data.loc[(matches & data.testset), 'activity'].tolist())\n",
    "\n",
    "            print(\"Overall\".upper().center(80))\n",
    "\n",
    "            test_y = np.hstack(ytrue_collector)\n",
    "            y_pred = np.hstack(ypred_collector)\n",
    "            print(metrics.classification_report(test_y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity to Zero-Originality Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O - Originality\n",
    "R - Resistance to Premature Closure\n",
    "E - Elaboration\n",
    "T - Abstractness of Titles\n",
    "F - Fluency (doesn't concern us because we're looking by prompt)\n",
    "C - Creativity Index (an additional metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/peter.organisciak/Documents/projects/motes-figural/notebooks/supervised-figural.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/peter.organisciak/Documents/projects/motes-figural/notebooks/supervised-figural.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m data[\u001b[39m'\u001b[39m\u001b[39mF\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalue_counts()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data['F'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e93f6973baf80d840d9d6715f091ca11c0bba5902c2f522aa0ee562f8ebeaf67"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
