{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ab7PsHos2N5G","executionInfo":{"status":"ok","timestamp":1673826819741,"user_tz":420,"elapsed":700,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"}},"outputId":"fd4a8cd3-be85-470f-e43b-5edfde1165f3"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"qW61O0DZ3qu7","executionInfo":{"status":"ok","timestamp":1673826827926,"user_tz":420,"elapsed":8186,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"}}},"outputs":[],"source":["!pip -q install transformers datasets evaluate wandb"]},{"cell_type":"markdown","source":["See the image classification [task page](https://huggingface.co/tasks/image-classification) for more information about its associated models, datasets, and metrics."],"metadata":{"id":"0SUSqRsw4MpZ"}},{"cell_type":"code","source":["from datasets import load_dataset, Image, Dataset, DatasetDict\n","from transformers import DefaultDataCollator, AutoImageProcessor\n","from pathlib import Path\n","\n","import torch.nn.functional as f\n","import torch\n","\n","import wandb\n","import pandas as pd\n","import numpy as np\n","wandb.login()"],"metadata":{"id":"xq64WQKg59bs","executionInfo":{"status":"ok","timestamp":1673826841912,"user_tz":420,"elapsed":13991,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"}},"outputId":"d71ebbcb-0588-4061-8635-03d08800a816","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mporg\u001b[0m (\u001b[33mmassive-texts\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["## Prep Dataset"],"metadata":{"id":"gJlma51PLnTG"}},{"cell_type":"code","source":["wd = Path('/content/drive/MyDrive/Projects/motes-figural')\n","project_data_dir = wd / 'data'\n","img_dir = wd / 'extracted_content'\n","gt_dir = wd / 'img_ground_truth'\n","\n","seed = 1234 #@param {type:\"integer\"}\n","name = \"motes_figural\" #@param {type:\"string\"}\n","classification_model = \"microsoft/beit-base-patch16-224-pt22k-ft22k\" #@param [\"google/vit-large-patch16-224\", \"microsoft/beit-large-patch16-224-pt22k-ft22k\", \"facebook/convnext-base-224\", \"microsoft/resnet-50\", \"google/vit-base-patch16-384\", \"microsoft/beit-base-patch16-224-pt22k-ft22k\", \"google/vit-base-patch16-224\"]\n","%env WANDB_PROJECT=$name\n","\n","modeldir = project_data_dir / 'models' / (name+'_'+classification_model.replace('/','-'))\n","modeldir.mkdir(exist_ok=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dSO2BydmJYUL","executionInfo":{"status":"ok","timestamp":1673826841913,"user_tz":420,"elapsed":5,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"}},"outputId":"c78ba106-fce3-4ae5-91ca-042e9c07379d"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["env: WANDB_PROJECT=motes_figural\n"]}]},{"cell_type":"markdown","source":["### General Prep"],"metadata":{"id":"pp2CuLA8KGDB"}},{"cell_type":"code","source":["rng = np.random.default_rng(seed=seed)\n","data = pd.read_csv(project_data_dir / 'figural_ground_truth.csv', index_col=0)\n","data = data[['img_path', 'booklet', 'id', 'pdf_path', 'titlepage', 'F', 'O', 'T', 'E', 'R', 'C', 'Name']]\n","# 10% for test set\n","test_prop = 0.1\n","val_prop = 0.04\n","\n","data['split'] = 'train'\n","randv = rng.random(size=len(data))\n","data.loc[(randv < test_prop+val_prop), 'split'] = 'val'\n","data.loc[(randv < test_prop), 'split'] = 'test'\n","print(\"Data split sizes\")\n","display((data.split.value_counts() / len(data)).round(2))\n","display(data.split.value_counts())\n","\n","data['img_path'] = data.img_path.apply(lambda x: Path(x))\n","data['testset'] = (rng.random(size=len(data)) < test_prop)\n","data['activity'] = data.img_path.apply(lambda x: x.parent.stem)\n","# id-encode activity\n","activities = data['activity'].unique().tolist()\n","id2activity = {i:x for i, x in enumerate(activities)}\n","activity2id = {x:i for i, x in enumerate(activities)}\n","data['activity_id'] = data.activity.replace(activity2id)\n","\n","for measure_data in ['sims_to_blank.parquet', 'avg_sims.parquet', 'elaboration.parquet', 'zlist_sims_sketch_of.parquet']:\n","    x = pd.read_parquet(project_data_dir / measure_data)\n","    x = x.drop(columns=[y for y in ['path', 'cropped', 'contrast'] if y in x.columns])\n","    data = data.merge(x)\n","# remove some data errors\n","data.loc[data['F'] > 1, 'F'] = np.NaN\n","data.loc[data['T'] > 3, 'T'] = np.NaN\n","data.loc[data['R'] > 2, 'R'] = np.NaN\n","\n","def fix_path(x):\n","    y= (project_data_dir / 'outputs' / x.booklet.lower() / x.img_path.parent.name / x.img_path.name)\n","    return y\n","data.img_path = data.apply(fix_path, axis=1)\n","\n","# ignore missing data\n","data = data[~data.O.isna()]\n","data.O = data.O.astype(int)\n","\n","data.sample(1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"6kFLT_hkKcC3","executionInfo":{"status":"ok","timestamp":1673826843016,"user_tz":420,"elapsed":1106,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"}},"outputId":"a3f0fd7e-5a93-4d84-df16-1e2e42203996"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Data split sizes\n"]},{"output_type":"display_data","data":{"text/plain":["train    0.86\n","test     0.10\n","val      0.04\n","Name: split, dtype: float64"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["train    4255\n","test      472\n","val       208\n","Name: split, dtype: int64"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/pandas/core/generic.py:5516: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  self[name] = value\n"]},{"output_type":"execute_result","data":{"text/plain":["                                               img_path   booklet  \\\n","2105  /content/drive/MyDrive/Projects/motes-figural/...  BOOKLETA   \n","\n","                    id                                           pdf_path  \\\n","2105  bb3e3-RD-SL-4811  ../data/ttct_figural/REDACTED/TTCT Post ASS SS...   \n","\n","      titlepage    F  O    T    E    R  ...    activity activity_id blank_sim  \\\n","2105          3  1.0  0  1.0  1.0  0.0  ...  activity2d           4  0.769583   \n","\n","      blank_sim_uncropped   avg_sim  avg_sim_uncropped  elaboration_raw  \\\n","2105             0.761269  0.786432           0.774703         0.115088   \n","\n","      min_zlist  mean_zlist  lowest3_zlist  \n","2105   0.223252    0.251545       0.231451  \n","\n","[1 rows x 24 columns]"],"text/html":["\n","  <div id=\"df-7c4ab854-f951-40dd-b71f-29ce0b298bb0\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>img_path</th>\n","      <th>booklet</th>\n","      <th>id</th>\n","      <th>pdf_path</th>\n","      <th>titlepage</th>\n","      <th>F</th>\n","      <th>O</th>\n","      <th>T</th>\n","      <th>E</th>\n","      <th>R</th>\n","      <th>...</th>\n","      <th>activity</th>\n","      <th>activity_id</th>\n","      <th>blank_sim</th>\n","      <th>blank_sim_uncropped</th>\n","      <th>avg_sim</th>\n","      <th>avg_sim_uncropped</th>\n","      <th>elaboration_raw</th>\n","      <th>min_zlist</th>\n","      <th>mean_zlist</th>\n","      <th>lowest3_zlist</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2105</th>\n","      <td>/content/drive/MyDrive/Projects/motes-figural/...</td>\n","      <td>BOOKLETA</td>\n","      <td>bb3e3-RD-SL-4811</td>\n","      <td>../data/ttct_figural/REDACTED/TTCT Post ASS SS...</td>\n","      <td>3</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>activity2d</td>\n","      <td>4</td>\n","      <td>0.769583</td>\n","      <td>0.761269</td>\n","      <td>0.786432</td>\n","      <td>0.774703</td>\n","      <td>0.115088</td>\n","      <td>0.223252</td>\n","      <td>0.251545</td>\n","      <td>0.231451</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1 rows Ã— 24 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7c4ab854-f951-40dd-b71f-29ce0b298bb0')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-7c4ab854-f951-40dd-b71f-29ce0b298bb0 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-7c4ab854-f951-40dd-b71f-29ce0b298bb0');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["TODO add activity to data (or train separate classifiers)\n","\n","For a single classifier, I would use the first few values of the image input as special characters which encode the activity. VIT uses a transformer architecture, maybe I could grab a 'tail' value.\n","\n","*although... if no cropping is done, the 'activity type' is just the consistently dark pixels.*"],"metadata":{"id":"br8TQv5XssMm"}},{"cell_type":"code","source":["# Create dataset\n","labels = [\"Not Original\", \"Original\"]\n","id2label = {str(i):x for i, x in enumerate(labels)}\n","label2id = {v:k for k,v in id2label.items()}\n","\n","# Create dataset\n","datasets = {}\n","for split in ['test', 'train', 'val']:\n","    subset = data[(data.split == split)]\n","    datasets[split] = Dataset.from_dict({'image': subset.img_path.astype(str),\n","                                         'activity': subset.activity_id.astype(int),\n","                                         'label': subset.O}).cast_column(\"image\", Image()).shuffle(seed=seed)\n","\n","rawdataset = DatasetDict(datasets)\n","rawdataset"],"metadata":{"id":"oprVNgIZ6lD_","executionInfo":{"status":"ok","timestamp":1673826843123,"user_tz":420,"elapsed":109,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"}},"outputId":"4074916d-4f1a-4a3d-c3cd-421b6cfba17b","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    test: Dataset({\n","        features: ['image', 'activity', 'label'],\n","        num_rows: 348\n","    })\n","    train: Dataset({\n","        features: ['image', 'activity', 'label'],\n","        num_rows: 3016\n","    })\n","    val: Dataset({\n","        features: ['image', 'activity', 'label'],\n","        num_rows: 142\n","    })\n","})"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["### Model-Specific Prep\n","\n","Transformations and normalization"],"metadata":{"id":"HdG0SfAZMOg5"}},{"cell_type":"code","source":["from torchvision.transforms import RandomResizedCrop, RandomHorizontalFlip, Resize, Compose, Normalize, ToTensor, RandomInvert\n","import warnings\n","\n","one_hot = False #@param {type:\"boolean\"}\n","if one_hot:\n","    warnings.warn(\"The current one-hot strategy likely won't make a difference because of the patches in VITs. A better strategy would be to change the token in the feature extraction\")\n","invert_prob = 0 #@param {type:\"slider\", min:0, max:1, step:0.5}\n","random_crop = False #@param {type:\"boolean\"}\n","random_hflip = False #@param {type:'boolean'}\n","\n","image_processor = AutoImageProcessor.from_pretrained(classification_model)\n","if \"shortest_edge\" in image_processor.size:\n","    size = image_processor.size[\"shortest_edge\"]\n","else:\n","    size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n","print(\"image size:\", size)\n","\n","pipeline = []\n","normalize = Normalize(mean=image_processor.image_mean,\n","                      std=image_processor.image_std)\n","\n","if random_crop:\n","    pipeline.append(RandomResizedCrop(size))\n","else:\n","    pipeline.append(Resize(size))\n","\n","if random_hflip:\n","    pipeline.append(RandomHorizontalFlip(p=0.5))\n","\n","if invert_prob > 0:\n","    pipeline.append(RandomInvert(p=invert_prob)) # possibly force it to look for signal beyond ink\n","\n","pipeline += [ToTensor(), normalize]\n","_transforms = Compose(pipeline)\n","\n","def one_hot_pad(input):\n","    pixel_values, activity = input\n","    pixel_values[:, :len(activities), 0] = f.one_hot(torch.tensor(activity), num_classes=len(activities))\n","    return pixel_values\n","\n","one_hot_transform = Compose([one_hot_pad])\n","\n","def transforms(examples, one_hot=False):\n","    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n","    if one_hot:\n","        examples[\"pixel_values\"] = [one_hot_transform((px, act)) for px, act in zip(examples['pixel_values'], examples['activity'])]\n","    del examples[\"image\"]\n","    del examples['activity']\n","    return examples\n","\n","dataset = rawdataset.with_transform(lambda x: transforms(x, one_hot))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"Bq9C_ieJMT-j","executionInfo":{"status":"ok","timestamp":1673826843682,"user_tz":420,"elapsed":561,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"}},"outputId":"0801d25b-db6f-4ef9-9ae9-7e2bea852dce"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"]},{"output_type":"stream","name":"stdout","text":["image size: (224, 224)\n"]}]},{"cell_type":"markdown","metadata":{"id":"J_K3Q8GI3qvD"},"source":["## Train"]},{"cell_type":"code","source":["import numpy as np\n","import evaluate\n","from transformers import AutoModelForImageClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n","\n","accuracy = evaluate.load(\"accuracy\")\n","f1 = evaluate.load('f1')\n","pearson = evaluate.load(\"pearsonr\")\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    # this is a classification problem, so use predictions rather than logits\n","    predictions = np.argmax(logits, axis=1)\n","    metrics = {}\n","    for metric in [accuracy, f1, pearson]:\n","        metrics.update(metric.compute(predictions=predictions, references=labels))\n","    for metric in [pearson]:\n","        softmax = torch.tensor(logits).softmax(dim=-1)[:, 1] # make a single 'probability of class 2' number, bound [0,1]\n","        r_logit = metric.compute(predictions=softmax.numpy(), references=labels.astype(np.float32))\n","        metrics['pearsonr_soft'] = r_logit['pearsonr']\n","\n","    return metrics"],"metadata":{"id":"-SdHWZ4aA_Gx","executionInfo":{"status":"ok","timestamp":1673826847167,"user_tz":420,"elapsed":3486,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2fpGvNf63qvD"},"source":["Your `compute_metrics` function is ready to go now, and you'll return to it when you setup your training."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"jqRtgj3E3qvD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673826850056,"user_tz":420,"elapsed":2892,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"}},"outputId":"76585baf-1002-419a-b6ec-346f02594fb6"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BeitForImageClassification were not initialized from the model checkpoint at microsoft/beit-base-patch16-224-pt22k-ft22k and are newly initialized because the shapes did not match:\n","- classifier.weight: found shape torch.Size([21841, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n","- classifier.bias: found shape torch.Size([21841]) in the checkpoint and torch.Size([2]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# Init model\n","model = AutoModelForImageClassification.from_pretrained(\n","    classification_model,\n","    num_labels=len(labels),\n","    id2label=id2label,\n","    label2id=label2id,\n","    ignore_mismatched_sizes=True # without this, it has the original classifier's n(classes) from ImageNet\n",")"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"g9QBgblI3qvD","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"f3365e41-78e7-450d-f5a1-3223900541e5","executionInfo":{"status":"ok","timestamp":1673827789681,"user_tz":420,"elapsed":939544,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href=\"https://wandb.me/wandb-init\" target=\"_blank\">the W&B docs</a>."]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.13.9"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20230115_235410-gtiliwc9</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href=\"https://wandb.ai/massive-texts/motes_figural/runs/gtiliwc9\" target=\"_blank\">balmy-firebrand-16</a></strong> to <a href=\"https://wandb.ai/massive-texts/motes_figural\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href=\"https://wandb.ai/massive-texts/motes_figural\" target=\"_blank\">https://wandb.ai/massive-texts/motes_figural</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href=\"https://wandb.ai/massive-texts/motes_figural/runs/gtiliwc9\" target=\"_blank\">https://wandb.ai/massive-texts/motes_figural/runs/gtiliwc9</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 3016\n","  Num Epochs = 25\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 256\n","  Gradient Accumulation steps = 4\n","  Total optimization steps = 300\n","  Number of trainable parameters = 85763522\n","Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='96' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 96/300 15:22 < 33:22, 0.10 it/s, Epoch 8/25]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Pearsonr</th>\n","      <th>Pearsonr Soft</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.633300</td>\n","      <td>0.565090</td>\n","      <td>0.732394</td>\n","      <td>0.819048</td>\n","      <td>0.330771</td>\n","      <td>0.366065</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.517700</td>\n","      <td>0.544242</td>\n","      <td>0.753521</td>\n","      <td>0.820513</td>\n","      <td>0.427345</td>\n","      <td>0.444930</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.462800</td>\n","      <td>0.510737</td>\n","      <td>0.816901</td>\n","      <td>0.870000</td>\n","      <td>0.564314</td>\n","      <td>0.531016</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.367800</td>\n","      <td>0.499610</td>\n","      <td>0.760563</td>\n","      <td>0.819149</td>\n","      <td>0.468130</td>\n","      <td>0.552677</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.281400</td>\n","      <td>0.541519</td>\n","      <td>0.788732</td>\n","      <td>0.852941</td>\n","      <td>0.488483</td>\n","      <td>0.556378</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.165900</td>\n","      <td>0.527205</td>\n","      <td>0.802817</td>\n","      <td>0.862745</td>\n","      <td>0.523604</td>\n","      <td>0.555676</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.122000</td>\n","      <td>0.623570</td>\n","      <td>0.725352</td>\n","      <td>0.786885</td>\n","      <td>0.410473</td>\n","      <td>0.530394</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.072100</td>\n","      <td>0.754464</td>\n","      <td>0.760563</td>\n","      <td>0.824742</td>\n","      <td>0.446964</td>\n","      <td>0.509206</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 142\n","  Batch size = 64\n","Saving model checkpoint to /content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-12\n","Configuration saved in /content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-12/config.json\n","Model weights saved in /content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-12/pytorch_model.bin\n","Image processor saved in /content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-12/preprocessor_config.json\n","Deleting older checkpoint [/content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-108] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 142\n","  Batch size = 64\n","Saving model checkpoint to /content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-24\n","Configuration saved in /content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-24/config.json\n","Model weights saved in /content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-24/pytorch_model.bin\n","Image processor saved in /content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-24/preprocessor_config.json\n","Deleting older checkpoint [/content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-156] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 142\n","  Batch size = 64\n","Saving model checkpoint to /content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-36\n","Configuration saved in /content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-36/config.json\n","Model weights saved in /content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-36/pytorch_model.bin\n","Image processor saved in /content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-36/preprocessor_config.json\n","Deleting older checkpoint [/content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-168] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 142\n","  Batch size = 64\n","Saving model checkpoint to /content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-48\n","Configuration saved in /content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-48/config.json\n","Model weights saved in /content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-48/pytorch_model.bin\n","Image processor saved in /content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-48/preprocessor_config.json\n","Deleting older checkpoint [/content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-12] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 142\n","  Batch size = 64\n","Saving model checkpoint to /content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-60\n","Configuration saved in /content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-60/config.json\n","Model weights saved in /content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-60/pytorch_model.bin\n","Image processor saved in /content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-60/preprocessor_config.json\n","Deleting older checkpoint [/content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-24] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 142\n","  Batch size = 64\n","Saving model checkpoint to /content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-72\n","Configuration saved in /content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-72/config.json\n","Model weights saved in /content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-72/pytorch_model.bin\n","Image processor saved in /content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-72/preprocessor_config.json\n","Deleting older checkpoint [/content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-48] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 142\n","  Batch size = 64\n","Saving model checkpoint to /content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-84\n","Configuration saved in /content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-84/config.json\n","Model weights saved in /content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-84/pytorch_model.bin\n","Image processor saved in /content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-84/preprocessor_config.json\n","Deleting older checkpoint [/content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-60] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 142\n","  Batch size = 64\n","Saving model checkpoint to /content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-96\n","Configuration saved in /content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-96/config.json\n","Model weights saved in /content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-96/pytorch_model.bin\n","Image processor saved in /content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-96/preprocessor_config.json\n","Deleting older checkpoint [/content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-72] due to args.save_total_limit\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Projects/motes-figural/data/models/motes_figural_microsoft-beit-base-patch16-224-pt22k-ft22k/checkpoint-36 (score: 0.8169014084507042).\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=96, training_loss=0.3496219921701898, metrics={'train_runtime': 935.8159, 'train_samples_per_second': 80.571, 'train_steps_per_second': 0.321, 'total_flos': 1.868927582490329e+18, 'train_loss': 0.3496219921701898, 'epoch': 8.0})"]},"metadata":{},"execution_count":10}],"source":["epochs = 25\n","log_times_per_epoch = 5 # approx. used to set logging_steps\n","\n","# this is based off the observed limit with my own gpu\n","if ('384' in classification_model) or (\"large\" in classification_model):\n","    # beit-large-224 and vit-large-224 have 307m params, whereas their base models have 86m\n","    # https://arxiv.org/pdf/2106.08254.pdf\n","    batch_size = 20\n","    gradient_accumulation_steps = 8\n","elif 'convnext-base-224' in classification_model:\n","    batch_size = 48\n","    gradient_accumulation_steps = 4\n","elif '224' in classification_model:\n","    batch_size = 64\n","    gradient_accumulation_steps = 4\n","else:\n","    batch_size = 16\n","    gradient_accumulation_steps = 4\n","\n","# init logging. No pretty names here, I prefer changing names in wandb console\n","wandb.init(project=name)\n","wandb.config.update({'one-hot':one_hot,\n","                     'classification_model':classification_model,\n","                     'seed':seed,\n","                     'invert_prob':invert_prob,\n","                     'random_crop':random_crop,\n","                     'random_hflip': random_hflip})\n","\n","data_collator = DefaultDataCollator() \n","training_args = TrainingArguments(\n","    output_dir=str(modeldir),\n","    remove_unused_columns=False,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    save_total_limit=3, #deletes oldest checkpoints\n","    \n","    learning_rate= 5e-5,\n","    per_device_train_batch_size=batch_size,\n","    gradient_accumulation_steps=gradient_accumulation_steps,\n","    per_device_eval_batch_size=batch_size,\n","    warmup_ratio=0.2, #% of total steps to warm up (higher is slower)\n","    \n","    num_train_epochs=epochs,\n","    # log {log_times_per_epoch}, for easier comparison across different runs with different steps configs\n","    logging_steps = int(dataset['train'].num_rows/batch_size/gradient_accumulation_steps//log_times_per_epoch),\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"accuracy\",\n","    greater_is_better=True, \n","    push_to_hub=False, # true if you want to share\n","    report_to='wandb',\n","    run_name=modeldir.name\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=dataset[\"train\"],\n","    eval_dataset=dataset[\"val\"],\n","    tokenizer=image_processor,\n","    compute_metrics=compute_metrics,\n","    callbacks = [EarlyStoppingCallback(early_stopping_patience = 5)]\n",")\n","\n","trainer.train()"]},{"cell_type":"code","source":["x[:3].round(3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"id":"-disMLOInip-","executionInfo":{"status":"ok","timestamp":1673827789681,"user_tz":420,"elapsed":18,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"}},"outputId":"d6100d58-919a-43a8-9e3b-ee6c9a68f001"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   min_zlist  mean_zlist  lowest3_zlist   booklet    activity           id\n","0      0.222       0.267          0.238  BOOKLETA  activity2b  94fd1-71561\n","1      0.210       0.254          0.232  BOOKLETA  activity2b  9cf64-79099\n","2      0.239       0.270          0.247  BOOKLETA  activity2b  c6e72-74881"],"text/html":["\n","  <div id=\"df-f2c8b148-a812-47f3-972f-344271473e95\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>min_zlist</th>\n","      <th>mean_zlist</th>\n","      <th>lowest3_zlist</th>\n","      <th>booklet</th>\n","      <th>activity</th>\n","      <th>id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.222</td>\n","      <td>0.267</td>\n","      <td>0.238</td>\n","      <td>BOOKLETA</td>\n","      <td>activity2b</td>\n","      <td>94fd1-71561</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.210</td>\n","      <td>0.254</td>\n","      <td>0.232</td>\n","      <td>BOOKLETA</td>\n","      <td>activity2b</td>\n","      <td>9cf64-79099</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.239</td>\n","      <td>0.270</td>\n","      <td>0.247</td>\n","      <td>BOOKLETA</td>\n","      <td>activity2b</td>\n","      <td>c6e72-74881</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f2c8b148-a812-47f3-972f-344271473e95')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-f2c8b148-a812-47f3-972f-344271473e95 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-f2c8b148-a812-47f3-972f-344271473e95');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["predictions, label_ids, metrics = trainer.predict(dataset['test'], metric_key_prefix='test')\n","metrics = {k.replace('test_', 'test/'):v for k,v in metrics.items()}\n","wandb.log(metrics)\n","print(metrics)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":108},"id":"FGD1X9MddQ11","executionInfo":{"status":"ok","timestamp":1673827796794,"user_tz":420,"elapsed":7128,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"}},"outputId":"e0deda77-ca52-4f76-aaa9-d5629f4de68d"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["***** Running Prediction *****\n","  Num examples = 348\n","  Batch size = 64\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{'test/loss': 0.5087286233901978, 'test/accuracy': 0.7614942528735632, 'test/f1': 0.8151447661469934, 'test/pearsonr': 0.4792395758975373, 'test/pearsonr_soft': 0.5239505471435327, 'test/runtime': 6.934, 'test/samples_per_second': 50.187, 'test/steps_per_second': 0.865}\n"]}]},{"cell_type":"markdown","source":["Accuracy after 15 epoches, early stopping=3:\n","- microsoft/resnet-50: 0.511 (best was epoch 1, wtf?)\n","- google/vit-base-patch16-384: 0.911 (best, stopped at epoch 10)\n","- microsoft/beit-base-patch16-224-pt22k-ft22k: 0.8555566 (best was 0.86, stopped at epoch 6, epoch 1 was already 0.83)\n","- google/vit-base-patch16-224: 0.867 (best was 0.87, stopped at epoch 10)\n"],"metadata":{"id":"XgYjh6_ETIUQ"}},{"cell_type":"markdown","metadata":{"id":"D5O7eseN3qvD"},"source":["<Tip>\n","\n","For a more in-depth example of how to finetune a model for image classification, take a look at the corresponding [PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n","\n","</Tip>"]},{"cell_type":"markdown","metadata":{"id":"YnINhJtk3qvD"},"source":["## Inference"]},{"cell_type":"markdown","source":["Example of using the model, in a `pipeline`:"],"metadata":{"id":"zqWCol-_aB9j"}},{"cell_type":"code","execution_count":13,"metadata":{"id":"kk9vzbuA3qvE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673827796795,"user_tz":420,"elapsed":16,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"}},"outputId":"bc72e388-5ecc-4471-ee1d-b69b37bb02c1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'label': 1,\n"," 'pixel_values': tensor([[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n","          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n","          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n","          ...,\n","          [0.9843, 0.9922, 0.9059,  ..., 1.0000, 1.0000, 1.0000],\n","          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n","          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n"," \n","         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n","          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n","          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n","          ...,\n","          [0.9843, 0.9922, 0.9059,  ..., 1.0000, 1.0000, 1.0000],\n","          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n","          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n"," \n","         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n","          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n","          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n","          ...,\n","          [0.9843, 0.9922, 0.9059,  ..., 1.0000, 1.0000, 1.0000],\n","          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n","          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]])}"]},"metadata":{},"execution_count":13}],"source":["# load example image\n","image = dataset['test']\n","image[0]"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"vgag5rE33qvE","outputId":"f50edde6-d189-440f-d8d0-22b9e88042cb","colab":{"base_uri":"https://localhost:8080/","height":675},"executionInfo":{"status":"error","timestamp":1673827798335,"user_tz":420,"elapsed":1554,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"}}},"outputs":[{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    942\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/test_model/resolve/main/config.json","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    410\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1066\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1068\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1375\u001b[0m     )\n\u001b[0;32m-> 1376\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    267\u001b[0m             )\n\u001b[0;32m--> 268\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-63c495d4-698791e26cf42e1a6d7cd8af)\n\nRepository Not Found for url: https://huggingface.co/test_model/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf the repo is private, make sure you are authenticated.\nInvalid username or password.","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-2fc6e437543d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpath_to_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"test_model\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"image-classification\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_to_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0mhub_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_commit_hash\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_from_pipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m         \u001b[0mhub_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_commit_hash\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    807\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name_or_path\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0mtrust_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trust_remote_code\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             \u001b[0moriginal_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    612\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m                 \u001b[0;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    615\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m                     \u001b[0mconfiguration_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    425\u001b[0m             \u001b[0;34mf\"{path_or_repo_id} is not a local folder and is not a valid model identifier \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0;34m\"listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: test_model is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`."]}],"source":["from transformers import pipeline\n","path_to_model = \"test_model\"\n","\n","classifier = pipeline(\"image-classification\", model=path_to_model)\n","classifier(image)"]},{"cell_type":"markdown","source":["Or manually, rather than via pipeline:"],"metadata":{"id":"jl6gBcKhakro"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Za5BNwTu3qvE","executionInfo":{"status":"aborted","timestamp":1673827798335,"user_tz":420,"elapsed":2,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"}}},"outputs":[],"source":["from transformers import AutoImageProcessor, AutoModelForImageClassification\n","import torch\n","\n","image_processor = AutoImageProcessor.from_pretrained(path_to_model)\n","inputs = image_processor(image, return_tensors=\"pt\") # return as pytorch\n","\n","model = AutoModelForImageClassification.from_pretrained(path_to_model)\n","with torch.no_grad():\n","    logits = model(**inputs).logits\n","\n","predicted_label = logits.argmax(-1).item()\n","model.config.id2label[predicted_label]"]}],"metadata":{"colab":{"provenance":[{"file_id":"1KOcfVOW4kvfZvoFrULuP_a_kPVdNqhTA","timestamp":1670723824023},{"file_id":"https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/image_classification.ipynb","timestamp":1670711660068}],"collapsed_sections":["pp2CuLA8KGDB","YnINhJtk3qvD"]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"},"gpuClass":"standard","accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}